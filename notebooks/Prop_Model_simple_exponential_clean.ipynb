{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "import datetime as dt\n",
    "from scipy.stats import uniform, norm\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-starter",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "[print(chr(c), end=\" \") for c in range(ord(\"α\"), ord(\"ω\")+1)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        self.params = dict()\n",
    "        \n",
    "    def sample(self, **kwargs):\n",
    "        samples = dict()\n",
    "        for (name, (dist, p)) in self.params.items():\n",
    "            if isinstance(p, dict):\n",
    "                dist = dist.dist(**p)\n",
    "            else:\n",
    "                dist = dist.dist(*p)\n",
    "            samples.update({name: dist.random(**kwargs)})\n",
    "        return samples\n",
    "    \n",
    "    def dist(self):\n",
    "        dists = dict()\n",
    "        for (name, (dist, p)) in self.params.items():\n",
    "            if isinstance(p, dict):\n",
    "                dist = dist(name, **p)\n",
    "            else:\n",
    "                dist = dist(name, *p)\n",
    "            dists.update({name: dist})\n",
    "        return dists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf7e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = Params()\n",
    "P.params.update(dict(\n",
    "    bias=(pm.Normal, (0, 1)),\n",
    "    slope=(pm.Normal, (0, 1)),\n",
    "    y_scale=(pm.Normal, (0, 5)),\n",
    "    base=(pm.Uniform, (1, 10)),\n",
    "    t_scale=(pm.Normal, (0, 5)),\n",
    "    t_shift=(pm.Normal, (0, 2)),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_true = P.sample()\n",
    "params_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddc9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0137ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c96e5a",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def drift_func(p, t):\n",
    "    y = p[\"bias\"] + (p[\"slope\"] * t) + p[\"y_scale\"] * p[\"base\"] ** (p[\"t_scale\"] * t - p[\"t_shift\"])\n",
    "    return y, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d1801",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(1000)/1000\n",
    "y, _ = drift_func(params_true, t)\n",
    "df = pd.DataFrame({\"t\": t, \"y\": y})\n",
    "plot_true = alt.Chart(df).mark_line().encode(x=\"t\", y=\"y\", color = alt.value(\"blue\"))\n",
    "plot_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3fb800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def exponential_drift(t):\n",
    "#     with m:\n",
    "#         bias = pm.Normal(\"bias\", 0, 1)\n",
    "#         slope = pm.Normal(\"slope\", 0, 1)\n",
    "#         y_scale = pm.Normal(\"y_scale\", 0, 5)\n",
    "#         base = pm.Uniform(\"base\", 1, 10)\n",
    "#         t_scale = pm.Normal(\"t_scale\", 0, 5)\n",
    "#         t_shift = pm.Normal(\"t_shift\", 0, 2)\n",
    "#         y = bias + (slope * t) + y_scale * base ** (t_scale * t - t_shift)\n",
    "        \n",
    "#     return y, bias, slope, y_scale, base, t_scale, t_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [t, obs] = df[[\"t\", \"y\"]].values.T\n",
    "\n",
    "m = pm.Model()\n",
    "\n",
    "with m:\n",
    "#     params_dist = P.dist()\n",
    "    _, d = drift_func(P.dist(), t)\n",
    "    \n",
    "    ε = pm.HalfCauchy('ε', 0.5, testval=1)\n",
    "    \n",
    "    pm.Normal('obs', \n",
    "                 mu=y,\n",
    "                 sd=ε,\n",
    "                 observed=df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-makeup",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with m:\n",
    "    trace = pm.sample(250, init=\"adapt_diag\")\n",
    "    pm.plot_trace(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: trace[k].mean() for k in params_true.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ad21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_true "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_hat = {k: trace[k].mean() for k in params_true.keys()}\n",
    "df[\"y_hat\"], _ = drift_func(params_hat, t)\n",
    "\n",
    "\n",
    "plot_hat = alt.Chart(df).mark_line().encode(x=\"t:Q\", y=\"hat:Q\", color = alt.value(\"red\"))\n",
    "plot_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.power(t[:,None], powers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.power(t[:, None] -.5, exponents)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_dot(X, np.array([1, 3 , 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6499e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "\n",
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350d892",
   "metadata": {},
   "source": [
    "## Are all DALI boxes (vestiging==Breda) in metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67815b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOXID = [\n",
    "    \"133.195-1\",\n",
    "    \"027.4090-1\",\n",
    "    \"133.134-1\",\n",
    "    \"178.518-1\",\n",
    "    \"053.746-1\",\n",
    "#     \"133.141-1\", NOT AVAILABLE\n",
    "    \"027.0380-1\",\n",
    "    \"133.301-1\",\n",
    "#     \"027.2100-1\", NOT AVAILABLE\n",
    "#     \"027.4390-1\", NOT AVAILABLE\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0765b",
   "metadata": {},
   "source": [
    "## Conclusion: \n",
    "7/10 Missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b41ff",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "df_sample = pd.DataFrame()\n",
    "for ID in BOXID:\n",
    "    df_sample = df_sample.append(\n",
    "        pd.read_parquet(path = f\"../data/raw/BOXID={ID}/L=sumli/\").assign(BOXID=ID)\n",
    "    )\n",
    "    \n",
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd0613e",
   "metadata": {},
   "source": [
    "## Is min or max value robust enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18024b6d",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# query data an reformat it\n",
    "n_extremes=2\n",
    "i=2\n",
    "\n",
    "df_top = df_sample.query(f\"TOP <= {n_extremes}\").rename(columns = {'TOP': 'EXTREME'})\n",
    "df_bottom = df_sample.query(f\"BOTTOM >= -{n_extremes}\").rename(columns = {'BOTTOM': 'EXTREME'})\n",
    "df_extreme = (\n",
    "    pd.concat([df_top, df_bottom], axis=0)\n",
    "    .drop(columns=[\"TOP\", \"BOTTOM\"])\n",
    "    .sort_values([\"YEAR\", \"WEEK\"])\n",
    ")\n",
    "df_extreme[\"DATE\"] =  df_extreme.apply(lambda d: dt.datetime.fromisocalendar(d[\"YEAR\"], d[\"WEEK\"], 1), axis=1)\n",
    "df_extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ead61",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# plot result\n",
    "input_dropdown = alt.binding_select(options=BOXID)\n",
    "selection = alt.selection_single(fields=['BOXID'], bind=input_dropdown, name='Select DALI on', init={\"BOXID\":BOXID[0]})\n",
    "opacity = alt.condition(selection, alt.value(1.0), alt.value(0.1))\n",
    "(\n",
    "    alt.Chart(df_extreme)\n",
    "    .mark_line(point=True)\n",
    "    .encode(\n",
    "        x=alt.X(\"DATE:T\", title=\"date\"),\n",
    "        y=alt.Y(\"VALUE:Q\", title=f\"P sumli\"),\n",
    "        color=alt.Color(\"EXTREME:N\", sort=list(range(1,10)) + list(range(-9,0, 1))),\n",
    "        opacity=opacity,\n",
    "        tooltip=alt.Tooltip([\"BOXID\", \"DATE:T\"])\n",
    "    )\n",
    "    .transform_filter(selection)\n",
    "    .add_selection(selection)\n",
    "    .properties(width=800)\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d1bfa",
   "metadata": {},
   "source": [
    "## Conclusion: \n",
    "- No need to take second extreme, min and max will do fine, since data is already 15 min average week extremes.\n",
    "- Data availability/history is something to keep in mind.\n",
    "- Grid alternations could be problematic if no metadata about it is logged..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5c697",
   "metadata": {},
   "source": [
    "# First model based on box \"133.134-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4485a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from scipy import stats\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d615e062",
   "metadata": {},
   "source": [
    "### Data Selection and prepping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa5ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_ID = \"133.134-1\"\n",
    "df_min = df_extreme.query(f\"BOXID == '{BOX_ID}' & EXTREME == -1\")\n",
    "df_max = df_extreme.query(f\"BOXID == '{BOX_ID}' & EXTREME == 1\")\n",
    "\n",
    "def format_ts(df):\n",
    "    df = df.set_index(\"DATE\")[\"VALUE\"].sort_index()\n",
    "    return df.resample('7D').interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e54175",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = format_ts(df_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4cd109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_dot(a, b):\n",
    "    \"\"\"\n",
    "    The theano dot product and NUTS sampler don't work with large matrices?\n",
    "    \n",
    "    :param a: (np matrix)\n",
    "    :param b: (theano vector)\n",
    "    \"\"\"\n",
    "    return (a * b[None, :]).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler():\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.min = data.min()\n",
    "        self.max = data.max()\n",
    "        \n",
    "    def check_fit(self):\n",
    "        if self.max is None:\n",
    "            raise Exception(\"Can't use transform without fit first!\")\n",
    "        \n",
    "    def transform(self, data):\n",
    "        self.check_fit()\n",
    "        return ((data - self.min) / (self.max - self.min)).astype(float)\n",
    "        \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        self.check_fit()\n",
    "        return data * (self.max - self.min) + self.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c733cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ts.reset_index()\n",
    "date_scaler, value_scaler = MinMaxScaler(), MinMaxScaler()\n",
    "\n",
    "\n",
    "df[\"SCALED_DATE\"] = date_scaler.fit_transform(df[\"DATE\"].apply(dt.datetime.timestamp))\n",
    "value_scaler.fit(df[\"VALUE\"])\n",
    "value_scaler.min = 0\n",
    "df[\"SCALED_VALUE\"] = value_scaler.transform(df[\"VALUE\"])\n",
    "# observed_index = ts.index.map(dt.datetime.timestamp)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c78ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_model(m, t, n_changepoints=2, changepoints_prior_scale=0.1, \n",
    "                growth_prior_scale=1, changepoint_range=0.8):\n",
    "    \"\"\"\n",
    "    The piecewise linear trend with changepoint implementation in PyMC3.\n",
    "    :param m: (pm.Model)\n",
    "    :param t: (np.array) MinMax scaled time.\n",
    "    :param n_changepoints: (int) The number of changepoints to model.\n",
    "    :param changepoint_prior_scale: (flt/ None) The scale of the Laplace prior on the delta vector.\n",
    "                                    If None, a hierarchical prior is set.\n",
    "    :param growth_prior_scale: (flt) The standard deviation of the prior on the growth.\n",
    "    :param changepoint_range: (flt) Proportion of history in which trend changepoints will be estimated. \n",
    "    :return g, A, s: (tt.vector, np.array, tt.vector)\n",
    "    \"\"\"\n",
    "    s = np.linspace(0, changepoint_range * np.max(t), n_changepoints + 1)[1:]\n",
    "    \n",
    "    # * 1 casts the boolean to integers\n",
    "    A = (t[:, None] > s) * 1\n",
    "\n",
    "    with m:\n",
    "        # initial growth\n",
    "        k = pm.Normal('k', 0 , growth_prior_scale)\n",
    "        \n",
    "        if changepoints_prior_scale is None:\n",
    "            changepoints_prior_scale = pm.Exponential('tau', 1.5)\n",
    "        \n",
    "        # rate of change\n",
    "        delta = pm.Laplace('delta', 0, changepoints_prior_scale, shape=n_changepoints)\n",
    "        # offset\n",
    "        m = pm.Normal('m', 0, 5)\n",
    "        gamma = -s * delta\n",
    "        \n",
    "        g = (k + det_dot(A, delta)) * t + (m + det_dot(A, gamma))\n",
    "    return g, A, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d0b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a PyMC3 Model context\n",
    "m = pm.Model()\n",
    "observed_timestamps = df[\"SCALED_DATE\"].values\n",
    "observed_values = df[\"SCALED_VALUE\"].values\n",
    "n_changepoints = 4\n",
    "\n",
    "with m:\n",
    "    y, A, s = trend_model(m, observed_timestamps, n_changepoints)\n",
    "    \n",
    "    sigma = pm.HalfCauchy('sigma', 0.5, testval=1)\n",
    "    pm.Normal('obs', \n",
    "                 mu=y,\n",
    "                 sd=sigma,\n",
    "                 observed=observed_values)\n",
    "    \n",
    "pm.model_to_graphviz(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(m, observations):\n",
    "    \"\"\"\n",
    "    :param m: (pm.Model)\n",
    "    :param df: (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    # Sample from the prior and check of the model is well defined.\n",
    "    y = pm.sample_prior_predictive(model=m)['obs']\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(y.mean(0), label='mean prior')\n",
    "    plt.fill_between(np.arange(y.shape[1]), -y.std(0), y.std(0), alpha=0.25, label='standard deviation')\n",
    "    plt.plot(observations, label='true value')\n",
    "    plt.legend()\n",
    "\n",
    "# And run the sanity check\n",
    "sanity_check(m, observed_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f44899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a point estimate of the models parameters\n",
    "with m:\n",
    "    aprox = pm.find_MAP()\n",
    "\n",
    "# Determine g, based on the parameters\n",
    "def det_trend(k, m, delta, t, s, A):\n",
    "    return (k + np.dot(A, delta)) * t + (m + np.dot(A, (-s * delta)))\n",
    "\n",
    "# run function and rescale to original scale\n",
    "g = det_trend(aprox['k'], aprox['m'], aprox['delta'], observed_timestamps, s, A)\n",
    "g_rescaled = value_scaler.inverse_transform(g)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.title('$g(t)$')\n",
    "plt.plot(g_rescaled)\n",
    "plt.scatter(np.arange(df.shape[0]), df[\"VALUE\"], s=0.5, color='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ee99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693e56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_series(t, p=52.1775, n=5):\n",
    "    # 2 pi n / p\n",
    "    x = 2 * np.pi * np.arange(1, n + 1) / p\n",
    "    # 2 pi n / p * t\n",
    "    x = x * t[:, None]\n",
    "    x = np.concatenate((np.cos(x), np.sin(x)), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weeks per year\n",
    "P = 52.1775\n",
    "P_scaled = P / len(df)\n",
    "def seasonality_model(m, timestamps, period=P_scaled, n=5, seasonality_prior_scale=10):\n",
    "    \n",
    "    x = fourier_series(timestamps, p=period, n=n)\n",
    "    with m:\n",
    "        beta = pm.Normal('beta_yearly', mu=0, sd=seasonality_prior_scale, shape=2 * n)\n",
    "    return x, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4476be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pm.Model()\n",
    "with m:\n",
    "    # changepoints_prior_scale is None, so the exponential distribution\n",
    "    # will be used as prior on \\tau.\n",
    "    y, A, s = trend_model(m, observed_timestamps, changepoints_prior_scale=None)\n",
    "    x_yearly, beta_yearly = seasonality_model(m, observed_timestamps, P_scaled, n=5)\n",
    "    \n",
    "    y += det_dot(x_yearly, beta_yearly)\n",
    "    \n",
    "    sigma = pm.HalfCauchy('sigma', 0.5, testval=1)\n",
    "    obs = pm.Normal('obs', \n",
    "                 mu=y, \n",
    "                 sd=sigma,\n",
    "                 observed=observed_values)\n",
    "pm.model_to_graphviz(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17474ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with m:\n",
    "    trace = pm.sample(500, return_inferencedata=False)\n",
    "# pm.traceplot(trace);\n",
    "pm.plot_trace(trace);"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b0771b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b684f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_seasonality_posterior(beta, x):\n",
    "    return np.dot(x, beta.T)\n",
    "\n",
    "p = 0.01\n",
    "# vector distributions\n",
    "beta_yearly = trace['beta_yearly']\n",
    "delta = trace['delta']\n",
    "\n",
    "# scalar distributions\n",
    "k = trace['k']\n",
    "m = trace['m']\n",
    "\n",
    "# determine the posterior by evaulating all the values in the trace.\n",
    "trend_posterior = ((k + np.dot(A, delta.T)) * observed_timestamps[:, None] + m + np.dot(A, (-s * delta).T))\n",
    "trend_posterior_rescaled = value_scaler.inverse_transform(trend_posterior)\n",
    "\n",
    "yearly_posterior = det_seasonality_posterior(beta_yearly, x_yearly)\n",
    "yearly_posterior_rescaled = value_scaler.inverse_transform(yearly_posterior)\n",
    "\n",
    "date = df['DATE'].dt.to_pydatetime()\n",
    "# sunday = np.argmax(df['ds'].dt.dayofweek)\n",
    "# weekdays = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n",
    "idx_year = np.argmax(df['DATE'].dt.dayofyear)\n",
    "\n",
    "plt.figure(figsize=(16, 3*6))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.title('total')\n",
    "total = trend_posterior_rescaled + yearly_posterior_rescaled\n",
    "quant_total = np.quantile(total, [p, 1 - p], axis=1)\n",
    "plt.fill_between(date, quant_total[0, :], quant_total[1, :], alpha=0.25)\n",
    "plt.plot(date, total.mean(1))\n",
    "plt.scatter(date, df['VALUE'], s=0.5, color='black')\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title('trend')\n",
    "quant_trend = np.quantile(trend_posterior_rescaled, [p, 1 - p], axis=1)\n",
    "plt.fill_between(date, quant_trend[0, :], quant_trend[1, :], alpha=0.25)\n",
    "plt.plot(date, trend_posterior_rescaled.mean(1))\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title('yearly')\n",
    "quant_yearly = np.quantile(yearly_posterior_rescaled, [p, 1 - p], axis=1)\n",
    "plt.fill_between(date[idx_year: idx_year + 52],\n",
    "                 quant_yearly[0, idx_year: idx_year + 52], quant_yearly[1, idx_year: idx_year + 52], alpha=0.25)\n",
    "plt.plot(date[idx_year: idx_year + 52], yearly_posterior_rescaled.mean(1)[idx_year: idx_year + 52]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418068a3",
   "metadata": {},
   "source": [
    "## Forecast attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = 25\n",
    "history_points = df.shape[0]\n",
    "probability_changepoint = n_changepoints / history_points\n",
    "\n",
    "total_period = pd.DataFrame(\n",
    "    {\n",
    "        \"DATE\": pd.date_range(\n",
    "            df[\"DATE\"].min(),\n",
    "            df[\"DATE\"].max() + pd.Timedelta(weeks, \"W\"),\n",
    "            freq=\"W\",\n",
    "        )\n",
    "    }\n",
    ")\n",
    "total_period[\"SCALED_DATE\"] = date_scaler.transform(total_period[\"DATE\"].apply(dt.datetime.timestamp))\n",
    "\n",
    "# vector distributions\n",
    "beta_yearly = trace['beta_yearly'] #.mean(0)\n",
    "n_samples = beta_yearly.shape[0]\n",
    "delta = trace['delta'].mean(0)\n",
    "\n",
    "# scalar distributions\n",
    "k = trace['k'].mean()\n",
    "m = trace['m'].mean()\n",
    "\n",
    "trend_forecast = []\n",
    "lambda_ = trace['tau'].mean()\n",
    "\n",
    "future = total_period[\"SCALED_DATE\"][total_period[\"SCALED_DATE\"] > 1].values\n",
    "\n",
    "\n",
    "for n in range(n_samples):\n",
    "    sample = np.random.random(future.shape)\n",
    "    new_changepoints = future[sample <= probability_changepoint]\n",
    "    new_delta = np.r_[delta, \n",
    "                      stats.laplace(0, lambda_).rvs(new_changepoints.shape[0])]\n",
    "    new_s = np.r_[s, new_changepoints]\n",
    "    new_A = (total_period[\"SCALED_DATE\"].values[:, None] > new_s) * 1\n",
    "\n",
    "    trend_forecast_sample = ((k + np.dot(new_A, new_delta)) * total_period[\"SCALED_DATE\"]  + (m + np.dot(new_A, (-new_s * new_delta))))\n",
    "    trend_forecast.append(trend_forecast_sample)\n",
    "    \n",
    "trend_forecast_rescaled = value_scaler.inverse_transform(np.array(trend_forecast).T)\n",
    "    \n",
    "# calculate seasonality for future\n",
    "new_x_yearly = fourier_series(total_period[\"SCALED_DATE\"].values, p=P_scaled)\n",
    "new_yearly_posterior = det_seasonality_posterior(beta_yearly, new_x_yearly)\n",
    "yearly_forecast_rescaled = value_scaler.inverse_transform(new_yearly_posterior)\n",
    "history = total_period[\"SCALED_DATE\"] <= 1\n",
    "yearly_forecast_rescaled[history, :] = yearly_forecast_rescaled[history, :].mean(1)[:,None].repeat(n_samples,1)\n",
    "    \n",
    "total_forecast_rescaled = trend_forecast_rescaled + yearly_forecast_rescaled\n",
    "\n",
    "date = total_period[\"DATE\"].dt.to_pydatetime()\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title('Forecast with uncertainty')\n",
    "plt.plot(date, total_forecast_rescaled.mean(1))\n",
    "quant = np.quantile(total_forecast_rescaled, [p, 1 - p], axis=1)\n",
    "plt.fill_between(date, quant[0, :], quant[1, :], alpha=0.25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6fa838",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = 25\n",
    "history_points = df.shape[0]\n",
    "probability_changepoint = n_changepoints / history_points\n",
    "\n",
    "total_period = pd.DataFrame(\n",
    "    {\n",
    "        \"DATE\": pd.date_range(\n",
    "            df[\"DATE\"].min(),\n",
    "            df[\"DATE\"].max() + pd.Timedelta(weeks, \"W\"),\n",
    "            freq=\"W\",\n",
    "        )\n",
    "    }\n",
    ")\n",
    "total_period[\"SCALED_DATE\"] = date_scaler.transform(total_period[\"DATE\"].apply(dt.datetime.timestamp))\n",
    "\n",
    "# vector distributions\n",
    "beta_yearly = trace['beta_yearly'] #.mean(0)\n",
    "n_samples = beta_yearly.shape[0]\n",
    "delta = trace['delta'].mean(0)\n",
    "\n",
    "# scalar distributions\n",
    "k = trace['k'].mean()\n",
    "m = trace['m'].mean()\n",
    "\n",
    "trend_forecast = []\n",
    "lambda_ = trace['tau'].mean()\n",
    "\n",
    "future = total_period[\"SCALED_DATE\"][total_period[\"SCALED_DATE\"] > 1].values\n",
    "\n",
    "\n",
    "for n in range(n_samples):\n",
    "    sample = np.random.random(future.shape)\n",
    "    new_changepoints = future[sample <= probability_changepoint]\n",
    "    new_delta = np.r_[delta, \n",
    "                      stats.laplace(0, lambda_).rvs(new_changepoints.shape[0])]\n",
    "    new_s = np.r_[s, new_changepoints]\n",
    "    new_A = (total_period[\"SCALED_DATE\"].values[:, None] > new_s) * 1\n",
    "\n",
    "    trend_forecast_sample = ((k + np.dot(new_A, new_delta)) * total_period[\"SCALED_DATE\"]  + (m + np.dot(new_A, (-new_s * new_delta))))\n",
    "    trend_forecast.append(trend_forecast_sample)\n",
    "    \n",
    "trend_forecast_rescaled = value_scaler.inverse_transform(np.array(trend_forecast).T)\n",
    "    \n",
    "# calculate seasonality for future\n",
    "new_x_yearly = fourier_series(total_period[\"SCALED_DATE\"].values, p=P_scaled)\n",
    "new_yearly_posterior = det_seasonality_posterior(beta_yearly, new_x_yearly)\n",
    "yearly_forecast_rescaled = value_scaler.inverse_transform(new_yearly_posterior)\n",
    "history = total_period[\"SCALED_DATE\"] <= 1\n",
    "yearly_forecast_rescaled[history, :] = yearly_forecast_rescaled[history, :].mean(1)[:,None].repeat(n_samples,1)\n",
    "    \n",
    "total_forecast_rescaled = trend_forecast_rescaled + yearly_forecast_rescaled\n",
    "\n",
    "date = total_period[\"DATE\"].dt.to_pydatetime()\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title('Forecast with uncertainty')\n",
    "plt.plot(date, total_forecast_rescaled.mean(1))\n",
    "quant = np.quantile(total_forecast_rescaled, [p, 1 - p], axis=1)\n",
    "plt.fill_between(date, quant[0, :], quant[1, :], alpha=0.25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60701cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c600574",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast = pd.DataFrame(\n",
    "    index=total_period[\"DATE\"][-len(future)-1:],\n",
    "    columns=[\"LOWER\", \"MEAN\", \"UPPER\"],\n",
    "    data=np.quantile(total_forecast_rescaled[-len(future)-1:], [p, .5, 1 - p], axis=1).T\n",
    ").reset_index()\n",
    "df_forecast.iloc[0, 1:] = np.repeat(df[\"VALUE\"].iloc[-1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_observed = (\n",
    "    alt.Chart(df)\n",
    "    .mark_line(point=True)\n",
    "    .encode(\n",
    "        x=alt.X(\"DATE:T\", title=\"date\"),\n",
    "        y=alt.Y(\"VALUE:Q\", title=f\"P sumli\"),\n",
    "        tooltip=alt.Tooltip([\"DATE:T\"])\n",
    "    )\n",
    "    .properties(width=800)\n",
    ").interactive()\n",
    "\n",
    "alt_forecasted_band = (\n",
    "    alt.Chart(df_forecast)\n",
    "    .mark_errorband(color=\"green\")\n",
    "    .encode(\n",
    "        x=alt.X(\"DATE:T\", title=\"date\"),\n",
    "        y=alt.Y(\"UPPER:Q\"),\n",
    "        y2=\"LOWER:Q\",\n",
    "    )\n",
    "    .properties(width=800)\n",
    ")\n",
    "\n",
    "alt_forecasted_mean = (\n",
    "    alt.Chart(df_forecast)\n",
    "    .mark_line(color=\"green\")\n",
    "    .encode(\n",
    "        x=alt.X(\"DATE:T\", title=\"date\"),\n",
    "        y=alt.Y(\"MEAN:Q\", title=f\"P sumli max\"),\n",
    "    )\n",
    "    .properties(width=800)\n",
    ").interactive()\n",
    "\n",
    "alt_observed + alt_forecasted_band + alt_forecasted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148f811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "662d7789",
   "metadata": {},
   "source": [
    "## with historical data plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #not using since offset !!!\n",
    "# weeks = 25\n",
    "# history_points = df.shape[0]\n",
    "# probability_changepoint = n_changepoints / history_points\n",
    "\n",
    "# total_period = pd.DataFrame(\n",
    "#     {\n",
    "#         \"DATE\": pd.date_range(\n",
    "#             df[\"DATE\"].min(),\n",
    "#             df[\"DATE\"].max() + pd.Timedelta(weeks, \"W\"),\n",
    "#             freq=\"W\",\n",
    "#         )\n",
    "#     }\n",
    "# )\n",
    "# total_period[\"SCALED_DATE\"] = date_scaler.transform(total_period[\"DATE\"].apply(dt.datetime.timestamp))\n",
    "\n",
    "# # vector distributions\n",
    "# beta_yearly = trace['beta_yearly'] #.mean(0)\n",
    "# n_samples = beta_yearly.shape[0]\n",
    "# delta = trace['delta'].mean(0)\n",
    "\n",
    "# # scalar distributions\n",
    "# k = trace['k'].mean()\n",
    "# m = trace['m'].mean()\n",
    "\n",
    "# trend_forecast = []\n",
    "# lambda_ = trace['tau'].mean()\n",
    "\n",
    "# future = total_period[\"SCALED_DATE\"][total_period[\"SCALED_DATE\"] > 1].values\n",
    "\n",
    "# for n in range(n_samples):\n",
    "#     sample = np.random.random(future.shape)\n",
    "#     new_s = future[sample <= probability_changepoint]\n",
    "#     new_delta = stats.laplace(0, lambda_).rvs(new_s.shape[0])\n",
    "#     new_A = (future[:, None] > new_s) * 1\n",
    "\n",
    "#     trend_forecast_sample = ((k + np.dot(new_A, new_delta)) * future  + (m + np.dot(new_A, (-new_s * new_delta))))\n",
    "#     trend_forecast.append(trend_forecast_sample)\n",
    "    \n",
    "# trend_forecast_rescaled = value_scaler.inverse_transform(np.array(trend_forecast).T)\n",
    "    \n",
    "# # calculate seasonality for future\n",
    "# new_x_yearly = fourier_series(future, p=P_scaled)\n",
    "# new_yearly_posterior = det_seasonality_posterior(beta_yearly, new_x_yearly)\n",
    "# yearly_forecast_rescaled = value_scaler.inverse_transform(new_yearly_posterior)\n",
    "    \n",
    "# total_forecast_rescaled = trend_forecast_rescaled + yearly_forecast_rescaled\n",
    "\n",
    "# date = total_period[\"DATE\"][total_period[\"SCALED_DATE\"] > 1].dt.to_pydatetime()\n",
    "# plt.figure(figsize=(16, 4))\n",
    "# plt.title('Forecast with uncertainty')\n",
    "# plt.plot(date, total_forecast_rescaled.mean(1))\n",
    "# quant = np.quantile(total_forecast_rescaled, [p, 1 - p], axis=1)\n",
    "# plt.fill_between(date, quant[0, :], quant[1, :], alpha=0.25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86351da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_forecast = (\n",
    "#     pd.DataFrame(\n",
    "#         index=total_period[total_period[\"SCALED_DATE\"] > 1][\"DATE\"],\n",
    "#         data=total_forecast_rescaled,\n",
    "#     )\n",
    "#     .melt(value_name=\"VALUE\", ignore_index=False)\n",
    "#     .drop(columns=\"variable\")\n",
    "#     .reset_index()\n",
    "# )\n",
    "# df_forecast.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast = pd.DataFrame(\n",
    "    index=total_period[total_period[\"SCALED_DATE\"] > 1][\"DATE\"],\n",
    "    columns=[\"LOWER\", \"MEAN\", \"UPPER\"],\n",
    "    data=np.quantile(total_forecast_rescaled, [p, .5, 1 - p], axis=1).T\n",
    ").reset_index()\n",
    "df_forecast.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4d5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_observed = (\n",
    "    alt.Chart(df)\n",
    "    .mark_line(point=True)\n",
    "    .encode(\n",
    "        x=alt.X(\"DATE:T\", title=\"date\"),\n",
    "        y=alt.Y(\"VALUE:Q\", title=f\"P sumli\"),\n",
    "        tooltip=alt.Tooltip([\"DATE:T\"])\n",
    "    )\n",
    "    .properties(width=800)\n",
    ").interactive()\n",
    "\n",
    "alt_forecasted_band = (\n",
    "    alt.Chart(df_forecast)\n",
    "    .mark_errorband(color=\"green\")\n",
    "    .encode(\n",
    "        x=alt.X(\"DATE:T\", title=\"date\"),\n",
    "        y=alt.Y(\"UPPER:Q\"),\n",
    "        y2=\"LOWER:Q\",\n",
    "    )\n",
    "    .properties(width=800)\n",
    ")\n",
    "\n",
    "alt_forecasted_mean = (\n",
    "    alt.Chart(df_forecast)\n",
    "    .mark_line(color=\"green\")\n",
    "    .encode(\n",
    "        x=alt.X(\"DATE:T\", title=\"date\"),\n",
    "        y=alt.Y(\"MEAN:Q\", title=f\"P sumli max\"),\n",
    "    )\n",
    "    .properties(width=800)\n",
    ").interactive()\n",
    "\n",
    "alt_observed + alt_forecasted_band + alt_forecasted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa97d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c911f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb250fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_observed = (\n",
    "    alt.Chart(df)\n",
    "    .mark_line(point=True)\n",
    "    .encode(\n",
    "        x=alt.X(\"DATE:T\", title=\"date\"),\n",
    "        y=alt.Y(\"VALUE:Q\", title=f\"P sumli\"),\n",
    "        tooltip=alt.Tooltip([\"DATE:T\"])\n",
    "    )\n",
    "    .properties(width=800)\n",
    ").interactive()\n",
    "\n",
    "alt_forecasted_mean = (\n",
    "    alt.Chart(df_forecast)\n",
    "    .mark_line(color=\"green\")\n",
    "    .encode(\n",
    "        x=alt.X(\"DATE:T\", title=\"date\"),\n",
    "        y=alt.Y(\"mean(VALUE):Q\", title=f\"P sumli max\"),\n",
    "#         tooltip=alt.Tooltip([\"DATE:T\"])\n",
    "    )\n",
    "    .properties(width=800)\n",
    ").interactive()\n",
    "\n",
    "alt_forecasted_band = (\n",
    "    alt.Chart(df_forecast)\n",
    "    .mark_errorband(extent='stdev', interpolate=\"linear\", color=\"green\")\n",
    "    .encode(\n",
    "        x=alt.X(\"DATE:T\", title=\"date\"),\n",
    "        y=alt.Y(\"VALUE:Q\", title=f\"P sumli max\"),\n",
    "#         tooltip=alt.Tooltip([\"DATE:T\"])\n",
    "    )\n",
    "    .properties(width=800)\n",
    ")\n",
    "\n",
    "alt_observed + alt_forecasted_ci + alt_forecasted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e34d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "line = alt.Chart(total_period).mark_line().encode(\n",
    "    x=alt.X(\"DATE:T\", title=\"date\"),\n",
    "    y=alt.Y(\"mean(VALUE):Q\", title=f\"P sumli\"),\n",
    ")\n",
    "\n",
    "band = alt.Chart(source).mark_errorband(extent='ci').encode(\n",
    "    x='Year',\n",
    "    y=alt.Y('Miles_per_Gallon', title='Miles/Gallon'),\n",
    ")\n",
    "\n",
    "band + line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08997422",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_observed = (\n",
    "    alt.Chart(df)\n",
    "    .mark_line(point=True)\n",
    "    .encode(\n",
    "        x=alt.X(\"DATE:T\", title=\"date\"),\n",
    "        y=alt.Y(\"VALUE:Q\", title=f\"P sumli\"),\n",
    "        tooltip=alt.Tooltip([\"DATE:T\"])\n",
    "    )\n",
    "    .properties(width=800)\n",
    ").interactive()\n",
    "\n",
    "alt_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b5316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c258167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:spark] *",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
