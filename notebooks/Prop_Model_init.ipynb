{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6499e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "\n",
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d984176",
   "metadata": {},
   "source": [
    "## Are all DALI boxes (vestiging==Breda) in metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67815b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOXID = [\n",
    "    \"133.195-1\",\n",
    "    \"027.4090-1\",\n",
    "    \"133.134-1\",\n",
    "    \"178.518-1\",\n",
    "    \"053.746-1\",\n",
    "#     \"133.141-1\", NOT AVAILABLE\n",
    "    \"027.0380-1\",\n",
    "    \"133.301-1\",\n",
    "#     \"027.2100-1\", NOT AVAILABLE\n",
    "#     \"027.4390-1\", NOT AVAILABLE\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51deb572",
   "metadata": {},
   "source": [
    "## Conclusion: \n",
    "7/10 Missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400069aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.DataFrame()\n",
    "for ID in BOXID:\n",
    "    df_sample = df_sample.append(\n",
    "        pd.read_parquet(path = f\"../data/raw/BOXID={ID}/L=sumli/\").assign(BOXID=ID)\n",
    "    )\n",
    "    \n",
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147172be",
   "metadata": {},
   "source": [
    "## Is min or max value robust enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18024b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query data an reformat it\n",
    "n_extremes=2\n",
    "i=2\n",
    "\n",
    "df_top = df_sample.query(f\"TOP <= {n_extremes}\").rename(columns = {'TOP': 'EXTREME'})\n",
    "df_bottom = df_sample.query(f\"BOTTOM >= -{n_extremes}\").rename(columns = {'BOTTOM': 'EXTREME'})\n",
    "df_extreme = (\n",
    "    pd.concat([df_top, df_bottom], axis=0)\n",
    "    .drop(columns=[\"TOP\", \"BOTTOM\"])\n",
    "    .sort_values([\"YEAR\", \"WEEK\"])\n",
    ")\n",
    "df_extreme[\"DATE\"] =  df_extreme.apply(lambda d: dt.datetime.fromisocalendar(d[\"YEAR\"], d[\"WEEK\"], 1), axis=1)\n",
    "df_extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ead61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "input_dropdown = alt.binding_select(options=BOXID)\n",
    "selection = alt.selection_single(fields=['BOXID'], bind=input_dropdown, name='Select DALI on', init={\"BOXID\":BOXID[0]})\n",
    "opacity = alt.condition(selection, alt.value(1.0), alt.value(0.1))\n",
    "(\n",
    "    alt.Chart(df_extreme)\n",
    "    .mark_line(point=True)\n",
    "    .encode(\n",
    "        x=alt.X(\"DATE:T\", title=\"date\"),\n",
    "        y=alt.Y(\"VALUE:Q\", title=f\"P sumli\"),\n",
    "        color=alt.Color(\"EXTREME:N\", sort=list(range(1,10)) + list(range(-9,0, 1))),\n",
    "        opacity=opacity,\n",
    "        tooltip=alt.Tooltip([\"BOXID\", \"DATE:T\"])\n",
    "    )\n",
    "    .transform_filter(selection)\n",
    "    .add_selection(selection)\n",
    "    .properties(width=800)\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488b49f",
   "metadata": {},
   "source": [
    "## Conclusion: \n",
    "- No need to take second extreme, min and max will do fine, since data is already 15 min average week extremes.\n",
    "- Data availability/history is something to keep in mind.\n",
    "- Grid alternations could be problematic if no metadata about it is logged..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a0ef3",
   "metadata": {},
   "source": [
    "# First model based on box \"133.134-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e4644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c60fa",
   "metadata": {},
   "source": [
    "### Data Selection and prepping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b8eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_ID = \"133.134-1\"\n",
    "df_min = df_extreme.query(f\"BOXID == '{BOX_ID}' & EXTREME == -1\")\n",
    "df_max = df_extreme.query(f\"BOXID == '{BOX_ID}' & EXTREME == 1\")\n",
    "\n",
    "def format_ts(df):\n",
    "    df = df.set_index(\"DATE\")[\"VALUE\"].sort_index()\n",
    "    return df.resample('7D').interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = format_ts(df_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_dot(a, b):\n",
    "    \"\"\"\n",
    "    The theano dot product and NUTS sampler don't work with large matrices?\n",
    "    \n",
    "    :param a: (np matrix)\n",
    "    :param b: (theano vector)\n",
    "    \"\"\"\n",
    "    return (a * b[None, :]).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler():\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.min = data.min()\n",
    "        self.max = data.max()\n",
    "        \n",
    "    def check_fit(self):\n",
    "        if self.max is None:\n",
    "            raise Exception(\"Can't use transform without fit first!\")\n",
    "        \n",
    "    def transform(self, data):\n",
    "        self.check_fit()\n",
    "        return ((data - self.min) / (self.max - self.min)).astype(float)\n",
    "        \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        self.check_fit()\n",
    "        return data * (self.max - self.min) + self.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5e2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d4f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ts.reset_index()\n",
    "date_scaler, value_scaler = MinMaxScaler(), MinMaxScaler()\n",
    "\n",
    "\n",
    "df[\"SCALED_DATE\"] = date_scaler.fit_transform(df[\"DATE\"].apply(dt.datetime.timestamp))\n",
    "value_scaler.fit(df[\"VALUE\"])\n",
    "value_scaler.min = 0\n",
    "df[\"SCALED_VALUE\"] = value_scaler.transform(df[\"VALUE\"])\n",
    "# observed_index = ts.index.map(dt.datetime.timestamp)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_model(m, t, n_changepoints=5, changepoints_prior_scale=0.2, \n",
    "                growth_prior_scale=5, changepoint_range=0.8):\n",
    "    \"\"\"\n",
    "    The piecewise linear trend with changepoint implementation in PyMC3.\n",
    "    :param m: (pm.Model)\n",
    "    :param t: (np.array) MinMax scaled time.\n",
    "    :param n_changepoints: (int) The number of changepoints to model.\n",
    "    :param changepoint_prior_scale: (flt/ None) The scale of the Laplace prior on the delta vector.\n",
    "                                    If None, a hierarchical prior is set.\n",
    "    :param growth_prior_scale: (flt) The standard deviation of the prior on the growth.\n",
    "    :param changepoint_range: (flt) Proportion of history in which trend changepoints will be estimated. \n",
    "    :return g, A, s: (tt.vector, np.array, tt.vector)\n",
    "    \"\"\"\n",
    "    s = np.linspace(0, changepoint_range * np.max(t), n_changepoints + 1)[1:]\n",
    "    \n",
    "    # * 1 casts the boolean to integers\n",
    "    A = (t[:, None] > s) * 1\n",
    "\n",
    "    with m:\n",
    "        # initial growth\n",
    "        k = pm.Normal('k', 0 , growth_prior_scale)\n",
    "        \n",
    "        if changepoints_prior_scale is None:\n",
    "            changepoints_prior_scale = pm.Exponential('tau', 1.5)\n",
    "        \n",
    "        # rate of change\n",
    "        delta = pm.Laplace('delta', 0, changepoints_prior_scale, shape=n_changepoints)\n",
    "        # offset\n",
    "        m = pm.Normal('m', 0, 5)\n",
    "        gamma = -s * delta\n",
    "        \n",
    "        g = (k + det_dot(A, delta)) * t + (m + det_dot(A, gamma))\n",
    "    return g, A, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f5a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a PyMC3 Model context\n",
    "m = pm.Model()\n",
    "observed_timestamps = df[\"SCALED_DATE\"].values\n",
    "observed_values = df[\"SCALED_VALUE\"].values\n",
    "\n",
    "with m:\n",
    "    y, A, s = trend_model(m, observed_timestamps)\n",
    "    \n",
    "    sigma = pm.HalfCauchy('sigma', 0.5, testval=1)\n",
    "    pm.Normal('obs', \n",
    "                 mu=y,\n",
    "                 sd=sigma,\n",
    "                 observed=observed_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37261512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(m, observations):\n",
    "    \"\"\"\n",
    "    :param m: (pm.Model)\n",
    "    :param df: (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    # Sample from the prior and check of the model is well defined.\n",
    "    y = pm.sample_prior_predictive(model=m)['obs']\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(y.mean(0), label='mean prior')\n",
    "    plt.fill_between(np.arange(y.shape[1]), -y.std(0), y.std(0), alpha=0.25, label='standard deviation')\n",
    "    plt.plot(observations, label='true value')\n",
    "    plt.legend()\n",
    "\n",
    "# And run the sanity check\n",
    "sanity_check(m, observed_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f369009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a point estimate of the models parameters\n",
    "with m:\n",
    "    aprox = pm.find_MAP()\n",
    "\n",
    "# Determine g, based on the parameters\n",
    "def det_trend(k, m, delta, t, s, A):\n",
    "    return (k + np.dot(A, delta)) * t + (m + np.dot(A, (-s * delta)))\n",
    "\n",
    "# run function and rescale to original scale\n",
    "g = det_trend(aprox['k'], aprox['m'], aprox['delta'], observed_timestamps, s, A)\n",
    "g_rescaled = value_scaler.inverse_transform(g)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.title('$g(t)$')\n",
    "plt.plot(g_rescaled)\n",
    "plt.scatter(np.arange(df.shape[0]), df[\"VALUE\"], s=0.5, color='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08700a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3449e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_series(t, p=52.1775, n=5):\n",
    "    # 2 pi n / p\n",
    "    x = 2 * np.pi * np.arange(1, n + 1) / p\n",
    "    # 2 pi n / p * t\n",
    "    x = x * t[:, None]\n",
    "    x = np.concatenate((np.cos(x), np.sin(x)), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weeks per year\n",
    "P = 52.1775\n",
    "P_scaled = P / len(df)\n",
    "def seasonality_model(m, timestamps, period=P_scaled, n=5, seasonality_prior_scale=10):\n",
    "    \n",
    "    x = fourier_series(timestamps, p=period, n=n)\n",
    "    with m:\n",
    "        beta = pm.Normal('beta_yearly', mu=0, sd=seasonality_prior_scale, shape=2 * n)\n",
    "    return x, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pm.Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe21166",
   "metadata": {},
   "outputs": [],
   "source": [
    "with m:\n",
    "    # changepoints_prior_scale is None, so the exponential distribution\n",
    "    # will be used as prior on \\tau.\n",
    "    y, A, s = trend_model(m, observed_timestamps, changepoints_prior_scale=None)\n",
    "    x_yearly, beta_yearly = seasonality_model(m, observed_timestamps, P_scaled, n=5)\n",
    "    \n",
    "    y += det_dot(x_yearly, beta_yearly)\n",
    "    \n",
    "    sigma = pm.HalfCauchy('sigma', 0.5, testval=1)\n",
    "    obs = pm.Normal('obs', \n",
    "                 mu=y, \n",
    "                 sd=sigma,\n",
    "                 observed=observed_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db4ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with m:\n",
    "    trace = pm.sample(500, return_inferencedata=False)\n",
    "# pm.traceplot(trace);\n",
    "pm.plot_trace(trace);"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79e90f53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_seasonality_posterior(beta, x):\n",
    "    return np.dot(x, beta.T)\n",
    "\n",
    "p = 0.005\n",
    "# vector distributions\n",
    "beta_yearly = trace['beta_yearly']\n",
    "delta = trace['delta']\n",
    "\n",
    "# scalar distributions\n",
    "k = trace['k']\n",
    "m = trace['m']\n",
    "\n",
    "# determine the posterior by evaulating all the values in the trace.\n",
    "trend_posterior = ((k + np.dot(A, delta.T)) * observed_timestamps[:, None] + m + np.dot(A, (-s * delta).T))\n",
    "trend_posterior_rescaled = value_scaler.inverse_transform(trend_posterior)\n",
    "\n",
    "yearly_posterior = det_seasonality_posterior(beta_yearly, x_yearly)\n",
    "yearly_posterior_rescaled = value_scaler.inverse_transform(yearly_posterior)\n",
    "\n",
    "date = df['DATE'].dt.to_pydatetime()\n",
    "# sunday = np.argmax(df['ds'].dt.dayofweek)\n",
    "# weekdays = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n",
    "idx_year = np.argmax(df['DATE'].dt.dayofyear)\n",
    "\n",
    "plt.figure(figsize=(16, 3*6))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.title('total')\n",
    "quant_total = np.quantile(trend_posterior_rescaled + yearly_posterior_rescaled, [p, 1 - p], axis=1)\n",
    "plt.fill_between(date, quant_total[0, :], quant_total[1, :], alpha=0.25)\n",
    "plt.plot(date, (trend_posterior_rescaled + yearly_posterior_rescaled).mean(1))\n",
    "plt.scatter(date, df['VALUE'], s=0.5, color='black')\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title('trend')\n",
    "quant_trend = np.quantile(trend_posterior_rescaled, [p, 1 - p], axis=1)\n",
    "plt.fill_between(date, quant_trend[0, :], quant_trend[1, :], alpha=0.25)\n",
    "plt.plot(date, trend_posterior_rescaled.mean(1))\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title('yearly')\n",
    "quant_yearly = np.quantile(yearly_posterior_rescaled, [p, 1 - p], axis=1)\n",
    "plt.fill_between(date[idx_year: idx_year + 52],\n",
    "                 quant_yearly[0, idx_year: idx_year + 52], quant_yearly[1, idx_year: idx_year + 52], alpha=0.25)\n",
    "plt.plot(date[idx_year: idx_year + 52], yearly_posterior_rescaled.mean(1)[idx_year: idx_year + 52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7f779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75697f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:spark] *",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
